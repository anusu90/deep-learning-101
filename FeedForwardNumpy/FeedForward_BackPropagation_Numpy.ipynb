{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "CcGnK8VlucUo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "gqGg7XRWvJCN",
    "outputId": "bc9e04cf-a52e-4a8d-c4b9-fb1f2ba58657"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "  f.seek(0)\n",
    "  training_data,validation_data,test_data = pickle.load(f,encoding='latin1')\n",
    "  f.close()\n",
    "  return training_data,validation_data,test_data\n",
    "training_data,validation_data,test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "1lmwcd0-0FHE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([5, 0, 4, ..., 8, 4, 8]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "ILEm3xdk0HYe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "wnpSysip0W0z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "Yr-7sdcx7EJc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def one_hot(j):\n",
    "# input is the target dataset of shape (m,) where m is the number of data points\n",
    "# returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1.0\n",
    "        index = index + 1\n",
    "    return new_array\n",
    " \n",
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(data.shape)\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "8hcetCN-7YT0"
   },
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "# Training data:\n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "# Validation data:\n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "# Test data:\n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, test_inputs, test_set_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "R8_J6jWY7eDn"
   },
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "UALCRSP_7npE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50000)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "-ygAZHVqHmMf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x[:][500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "Z8B60bOYKL4-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 50000)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "duKeM1zAKbrn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6fb492b6d0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANZElEQVR4nO3db6xU9Z3H8c9nFXwAGEGDy1rcdqskq2tW/oSYtKwuWiLyAJqggQfGjWRpIpoaIS6yMfXfA3W3kn1UpdaUrlXTpFVMxN2SmyauUZuLyCqUtLLItpQbsEtiKRpR/O6De2hu8c5vLjNn/nC/71cymZnznTPn63g/nDPzOzM/R4QAjH9/1usGAHQHYQeSIOxAEoQdSIKwA0mc3c2N2eajf6DDIsKjLW9rz277etu/tL3X9vp2ngtAZ7nVcXbbZ0n6laSvSTogaVDSyoj4RWEd9uxAh3Vizz5f0t6I2BcRxyU9J2lpG88HoIPaCftFkn4z4v6BatmfsL3a9nbb29vYFoA2tfMB3WiHCp87TI+ITZI2SRzGA73Uzp79gKSZI+5/QdLB9toB0CnthH1Q0qW2v2R7oqQVkl6spy0AdWv5MD4iPrV9u6T/lHSWpKciYndtnQGoVctDby1tjPfsQMd15KQaAGcOwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASLc/PLkm290s6KumEpE8jYl4dTQGoX1thr/x9RPyuhucB0EEcxgNJtBv2kPRT22/aXj3aA2yvtr3d9vY2twWgDY6I1le2/yIiDtqeLmmbpDsi4pXC41vfGIAxiQiPtrytPXtEHKyuD0t6XtL8dp4PQOe0HHbbk2xPOXlb0iJJu+pqDEC92vk0/kJJz9s++TzPRMR/1NJVD8yaNatYf+KJJxrWBgcHi+s+9thjLfV00vLly4v1iy++uGHt8ccfL667b9++lnrCmaflsEfEPkl/W2MvADqIoTcgCcIOJEHYgSQIO5AEYQeSaOsMutPeWB+fQbdo0aJifevWrS0/dzU82VA3/x+c6plnninWm/13v/TSS8X60aNHT7sntKcjZ9ABOHMQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLNX5s6dW6wPDAw0rE2ePLm4brNx9mZj0a+//nqxXnL11VcX6+ecc06x3uzvY8eOHcX6q6++2rB2zz33FNf9+OOPi3WMjnF2IDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCcfYxuuSSSxrWFixYUFz3rrvuKtY/+eSTYn3OnDnFeslll11WrF977bXF+nXXXVesL1my5LR7OmnPnj3F+ooVK4r13bt3t7zt8YxxdiA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2LpgyZUqxPmHChGL9yJEjdbZzWpr1Nnv27GL93nvvbVhbvHhxcd39+/cX66VzHzJreZzd9lO2D9veNWLZNNvbbL9bXU+ts1kA9RvLYfz3JV1/yrL1kgYi4lJJA9V9AH2sadgj4hVJpx5HLpW0ubq9WdKyetsCULezW1zvwogYkqSIGLI9vdEDba+WtLrF7QCoSathH7OI2CRpk5T3AzqgH7Q69HbI9gxJqq4P19cSgE5oNewvSrqlun2LpC31tAOgU5qOs9t+VtI1ki6QdEjStyS9IOlHki6W9GtJN0ZE08FgDuPzufzyyxvWXnvtteK65557brF+8803F+tPP/10sT5eNRpnb/qePSJWNiiVf/UAQF/hdFkgCcIOJEHYgSQIO5AEYQeS6PgZdMit9HPPx44dK67bbCpsnB727EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPs6KjSlM/nnXdecd3jx48X60NDQ620lBZ7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnF2dNTChQsb1iZOnFhc99Zbby3WBwYGWuopK/bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BE0ymba90YUzaPO+vWrSvWH3rooYa1nTt3Fte96qqrWmkpvUZTNjfds9t+yvZh27tGLLvP9m9t76wuN9TZLID6jeUw/vuSrh9l+caIuLK6bK23LQB1axr2iHhF0pEu9AKgg9r5gO52229Xh/lTGz3I9mrb221vb2NbANrUati/I+nLkq6UNCTp240eGBGbImJeRMxrcVsAatBS2CPiUESciIjPJH1X0vx62wJQt5bCbnvGiLtfl7Sr0WMB9Iem32e3/aykayRdYPuApG9Jusb2lZJC0n5J3+hci+ikKVOmFOvLly8v1m+77bZi/Y033mhYW7JkSXFd1Ktp2CNi5SiLv9eBXgB0EKfLAkkQdiAJwg4kQdiBJAg7kAQ/JT0OzJo1q2FtwYIFxXXvuOOOYv38888v1gcHB4v1VatWNawdO3asuC7qxZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Lgp6THgbfeeqth7Yorriiu+8EHHxTra9asKdafe+65Yh3d1/JPSQMYHwg7kARhB5Ig7EAShB1IgrADSRB2IAnG2ceBZcuWNaxt2LChuO7cuXOL9Q8//LBY37t3b7F+//33N6y98MILxXXRGsbZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnHuUmTJhXrN954Y7H+5JNPtrX9jz76qGHtpptuKq778ssvt7XtrFoeZ7c90/bPbO+xvdv2N6vl02xvs/1udT217qYB1Gcsh/GfSlobEX8t6SpJa2xfJmm9pIGIuFTSQHUfQJ9qGvaIGIqIHdXto5L2SLpI0lJJm6uHbZa0rEM9AqjBac31ZvuLkmZL+rmkCyNiSBr+B8H29AbrrJa0us0+AbRpzGG3PVnSjyXdGRG/t0f9DOBzImKTpE3Vc/ABHdAjYxp6sz1Bw0H/YUT8pFp8yPaMqj5D0uHOtAigDk2H3jy8C98s6UhE3Dli+b9I+r+IeNj2eknTIuLuJs/Fnv0MM336qO/O/mjLli3F+pw5cxrWzj67fGD54IMPFuuPPPJIsV4a9hvPGg29jeUw/iuSbpb0ju2d1bINkh6W9CPbqyT9WlJ5wBZATzUNe0S8KqnRG/Rr620HQKdwuiyQBGEHkiDsQBKEHUiCsANJ8BVXdNTddzc+9eKBBx4orjthwoRifd26dcX6xo0bi/Xxip+SBpIj7EAShB1IgrADSRB2IAnCDiRB2IEkGGdHz6xdu7ZYf/TRR4v1o0ePFusLFy5sWNuxY0dx3TMZ4+xAcoQdSIKwA0kQdiAJwg4kQdiBJAg7kATj7OhbJ06cKNab/e0uXry4YW3btm0t9XQmYJwdSI6wA0kQdiAJwg4kQdiBJAg7kARhB5JoOour7ZmSfiDpzyV9JmlTRPyb7fsk/aOk96uHboiIrZ1qFDjV+++/X6y/9957XerkzDCW+dk/lbQ2InbYniLpTdsnz0jYGBH/2rn2ANRlLPOzD0kaqm4ftb1H0kWdbgxAvU7rPbvtL0qaLenn1aLbbb9t+ynbUxuss9r2dtvb22sVQDvGHHbbkyX9WNKdEfF7Sd+R9GVJV2p4z//t0daLiE0RMS8i5rXfLoBWjSnstidoOOg/jIifSFJEHIqIExHxmaTvSprfuTYBtKtp2G1b0vck7YmIx0YsnzHiYV+XtKv+9gDUpelXXG1/VdJ/SXpHw0NvkrRB0koNH8KHpP2SvlF9mFd6Lr7iCnRYo6+48n12YJzh++xAcoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkxvLrsnX6naT/HXH/gmpZP+rX3vq1L4neWlVnb3/ZqNDV77N/buP29n79bbp+7a1f+5LorVXd6o3DeCAJwg4k0euwb+rx9kv6tbd+7Uuit1Z1pbeevmcH0D293rMD6BLCDiTRk7Dbvt72L23vtb2+Fz00Ynu/7Xds7+z1/HTVHHqHbe8asWya7W22362uR51jr0e93Wf7t9Vrt9P2DT3qbabtn9neY3u37W9Wy3v62hX66srr1vX37LbPkvQrSV+TdEDSoKSVEfGLrjbSgO39kuZFRM9PwLD9d5L+IOkHEfE31bJHJR2JiIerfyinRsQ/9Ulv90n6Q6+n8a5mK5oxcppxScsk/YN6+NoV+rpJXXjderFnny9pb0Tsi4jjkp6TtLQHffS9iHhF0pFTFi+VtLm6vVnDfyxd16C3vhARQxGxo7p9VNLJacZ7+toV+uqKXoT9Ikm/GXH/gPprvveQ9FPbb9pe3etmRnHhyWm2quvpPe7nVE2n8e6mU6YZ75vXrpXpz9vVi7CPNjVNP43/fSUi5khaLGlNdbiKsRnTNN7dMso0432h1enP29WLsB+QNHPE/S9IOtiDPkYVEQer68OSnlf/TUV96OQMutX14R7380f9NI33aNOMqw9eu15Of96LsA9KutT2l2xPlLRC0os96ONzbE+qPjiR7UmSFqn/pqJ+UdIt1e1bJG3pYS9/ol+m8W40zbh6/Nr1fPrziOj6RdINGv5E/n8k/XMvemjQ119J+u/qsrvXvUl6VsOHdZ9o+IholaTzJQ1Iere6ntZHvf27hqf2flvDwZrRo96+quG3hm9L2lldbuj1a1foqyuvG6fLAklwBh2QBGEHkiDsQBKEHUiCsANJEHYgCcIOJPH/1QtSh0fL7+IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k=train_set_x[:,500].reshape(28,28)\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gX81pJ8qKlN7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdLstybYKqbf"
   },
   "source": [
    "#### SIGMOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "Ht-qNzItLMD6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "  H = 1/(1+np.exp(-Z))\n",
    "  sigmoid_memory = Z\n",
    "  return H,sigmoid_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "oM_1JG4dLjSW"
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    H = np.maximum(0,Z)\n",
    "    assert(H.shape == Z.shape)\n",
    "    relu_memory = Z \n",
    "    return H, relu_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJAoEcDQMFYy"
   },
   "source": [
    "#### Softman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "KlRSrYKTNaJ1"
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "# Z is NumPy array of shape (n, m) where n is the number of neurons in the layer and m is the number of samples \n",
    "    Z_exp = np.exp(Z)\n",
    " \n",
    "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
    "    \n",
    "    H = Z_exp/Z_sum  #normalising step\n",
    "    softmax_memory = Z\n",
    "\n",
    "# softmax_memory is stored as it is used later on in backpropagation\n",
    "    return H, softmax_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dimensions):\n",
    "# dimensions is a list containing the number of neuron in each layer in the network\n",
    "# It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    " \n",
    "    np.random.seed(2)\n",
    "    parameters = {}\n",
    "    L = len(dimensions)            # number of layers in the network + 1\n",
    " \n",
    "    for l in range(1, L): \n",
    "        parameters['W'+str(l)]=np.random.randn(dimensions[l],dimensions[l-1])*0.1\n",
    "        parameters['b' + str(l)]=np.zeros((dimensions[l], 1)) \n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape==(dimensions[l],dimensions[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
      "  -0.09464469]\n",
      " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
      "   0.01179136]\n",
      " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
      "   0.00545832]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
      " [-0.28074571 -0.13967752  0.02641189]\n",
      " [ 0.10925169  0.06646016  0.08565535]\n",
      " [-0.11058228  0.03715795  0.13440124]\n",
      " [-0.16421272 -0.1153127   0.02013163]\n",
      " [ 0.13985659  0.07228733 -0.10717236]\n",
      " [-0.05673344 -0.03663499 -0.15460347]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Declare the dimensions:\n",
    "dimensions  = [784, 3, 7, 10]\n",
    " \n",
    "# Run the initialize_parameters() function:\n",
    "parameters = initialize_parameters(dimensions)\n",
    " \n",
    "# Print the resultant weights and biases:\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-0.04167578, -0.00562668, -0.21361961, ..., -0.06168445,\n",
       "          0.03213358, -0.09464469],\n",
       "        [-0.05301394, -0.1259207 ,  0.16775441, ..., -0.03284246,\n",
       "         -0.05623108,  0.01179136],\n",
       "        [ 0.07386378, -0.15872956,  0.01532001, ..., -0.08428557,\n",
       "          0.10040469,  0.00545832]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.06650944, -0.19626047,  0.2112715 ],\n",
       "        [-0.28074571, -0.13967752,  0.02641189],\n",
       "        [ 0.10925169,  0.06646016,  0.08565535],\n",
       "        [-0.11058228,  0.03715795,  0.13440124],\n",
       "        [-0.16421272, -0.1153127 ,  0.02013163],\n",
       "        [ 0.13985659,  0.07228733, -0.10717236],\n",
       "        [-0.05673344, -0.03663499, -0.15460347]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[ 0.20406947, -0.04960206, -0.06131668, -0.17449682,  0.01840741,\n",
       "         -0.00795452,  0.12406296],\n",
       "        [ 0.08625738, -0.01239074,  0.05854764,  0.19336815, -0.07322525,\n",
       "         -0.039205  ,  0.07512152],\n",
       "        [-0.05947453,  0.04753173, -0.1083593 ,  0.08228398,  0.07047718,\n",
       "         -0.05854343, -0.12699409],\n",
       "        [ 0.03255273,  0.00457369, -0.13662463,  0.10202692, -0.07310626,\n",
       "          0.1496712 ,  0.13433165],\n",
       "        [ 0.02566371,  0.0734615 , -0.14332651,  0.00178312,  0.05686418,\n",
       "         -0.1263975 , -0.14590294],\n",
       "        [ 0.15906599,  0.04026281,  0.14249133,  0.10019812, -0.28192685,\n",
       "         -0.11228612, -0.01523209],\n",
       "        [ 0.00556535,  0.01378749, -0.0675063 , -0.00885622, -0.10151087,\n",
       "          0.12861383, -0.09708002],\n",
       "        [-0.0577768 ,  0.08917285, -0.05625892,  0.01765442, -0.09055266,\n",
       "         -0.00368937,  0.04094553],\n",
       "        [-0.15298018, -0.16785625, -0.116733  ,  0.08260156,  0.05470732,\n",
       "          0.08330186,  0.14913897],\n",
       "        [-0.04016882, -0.07274709, -0.01175106,  0.0241847 ,  0.10988869,\n",
       "          0.01330499,  0.05696497]]),\n",
       " 'b3': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_forward(H_prev,W,b,activation='relu'):\n",
    "    if(activation == 'sigmoid'):\n",
    "        Z = np.dot(W,H_prev) + b\n",
    "        linear_memory = (H_prev,W,b)\n",
    "        H,activation_memory = sigmoid(Z)\n",
    "        \n",
    "    elif (activation == 'softmax'):\n",
    "        Z = np.dot(W,H_prev) + b\n",
    "        linear_memory = (H_prev,W,b)\n",
    "        H,activation_memory = softmax(Z)\n",
    "    \n",
    "    elif (activation == 'relu'):\n",
    "        Z = np.dot(W,H_prev) + b\n",
    "        linear_memory = (H_prev,W,b)\n",
    "        H,activation_memory = softmax(Z)\n",
    "        \n",
    "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
    "    memory = (linear_memory, activation_memory)\n",
    "    \n",
    "    return H,memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    " \n",
    "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
       "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "RFWJdZh1sl3m"
   },
   "outputs": [],
   "source": [
    "def L_layer_forward(X,parameters):\n",
    "    memories =[]\n",
    "    H=X\n",
    "    L=len(parameters)//2\n",
    "    \n",
    "    for l in range (1,L):\n",
    "        H_prev = H\n",
    "        H, memory = layer_forward(H_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation='relu')\n",
    "        memories.append(memory)\n",
    "    \n",
    "    HL, memory = layer_forward(H,parameters['W'+str(L)],parameters['b'+str(L)], activation ='softmax')\n",
    "    memories.append(memory)\n",
    "    \n",
    "\n",
    "    assert(HL.shape == (10, X.shape[1]))\n",
    "            \n",
    "    return HL, memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "8LqHKgQTs0dV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "[[0.10065537 0.10047675 0.10057191 0.10065854 0.10080465]\n",
      " [0.10458143 0.10453227 0.10447326 0.10461402 0.10450583]\n",
      " [0.09793931 0.09787829 0.09768822 0.09803077 0.09767624]\n",
      " [0.10266385 0.10281203 0.10294927 0.10257749 0.10284042]\n",
      " [0.09664777 0.09641897 0.09615637 0.09680893 0.09629539]\n",
      " [0.10129709 0.10108803 0.10100011 0.10137893 0.10119529]\n",
      " [0.09817199 0.09821798 0.09830936 0.09812712 0.09829453]\n",
      " [0.09884982 0.0989387  0.09893801 0.09883005 0.09884046]\n",
      " [0.09831046 0.09865039 0.09886287 0.09814423 0.09858055]\n",
      " [0.1008829  0.10098657 0.10105062 0.10082993 0.10096666]]\n"
     ]
    }
   ],
   "source": [
    "x_sample = train_set_x[:, 10:20]\n",
    "print(x_sample.shape)\n",
    "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
    "print(HL[:, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(HL, Y):\n",
    "# HL is probability matrix of shape (10, number of examples)\n",
    "# Y is true \"label\" vector shape (10, number of examples)\n",
    "    m = Y.shape[1]\n",
    " \n",
    "# loss is the cross-entropy loss\n",
    "    loss = (-1./ m) * np.sum(np.multiply(Y, np.log(HL)))\n",
    "    loss = np.squeeze(loss)\n",
    "      \n",
    "# To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dH, sigmoid_memory):\n",
    "# Implement the backpropagation of a sigmoid function\n",
    "# dH is the gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
    "# sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = sigmoid_memory \n",
    "    H = 1/(1+np.exp(-Z))\n",
    "    dZ = dH * H * (1-H)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dH, relu_memory):\n",
    "# Implement the backpropagation of a relu function\n",
    "# dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
    "# relu_memory is the memory stored in the sigmoid(Z) calculation\n",
    "    \n",
    "    Z = relu_memory\n",
    "    dZ = np.array(dH, copy=True) \n",
    "# dZ will be the same as dA wherever the elements of A weren't 0\n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_backward(dH, memory, activation = 'relu'):\n",
    "# takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
    "# performs the backprop depending upon the activation function\n",
    "    \n",
    "    linear_memory, activation_memory = memory\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dH, activation_memory)\n",
    "        H_prev, W, b = linear_memory\n",
    "        m = H_prev.shape[1]\n",
    "        dW = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "        db = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dH_prev = np.dot(linear_memory[1].T, dZ)\n",
    "    \n",
    "    return dH_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH_prev is \n",
      " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
      " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
      "dW is \n",
      " [[1.67565336 1.56891359]\n",
      " [1.39137819 1.4143854 ]\n",
      " [1.3597389  1.43013369]]\n",
      "db is \n",
      " [[0.37345476]\n",
      " [0.34414727]\n",
      " [0.29074635]]\n"
     ]
    }
   ],
   "source": [
    "# l-1 has two neurons, l has three, m = 5\n",
    "# H_prev is (l-1, m)\n",
    "# W is (l, l-1)\n",
    "# b is (l, 1)\n",
    "# H should be (l, m)\n",
    "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
    "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
    "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
    " \n",
    "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
    "np.random.seed(2)\n",
    "dH = np.random.rand(3,5)\n",
    "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
    "print('dH_prev is \\n' , dH_prev)\n",
    "print('dW is \\n' ,dW)\n",
    "print('db is \\n', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_backward(HL, Y, memories):\n",
    "# Takes the predicted value HL and the true target value Y and the \n",
    "# memories calculated by L_layer_forward as input\n",
    "# returns the gradients calculated for all the layers as a dict\n",
    " \n",
    "    gradients = {}\n",
    "    L = len(memories) # the number of layers\n",
    "    m = HL.shape[1]\n",
    "    Y= Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "# Perform the backprop for the last layer that is the softmax layer\n",
    "    current_memory = memories[-1]\n",
    "    linear_memory, activation_memory = current_memory\n",
    "    dZ = HL - Y\n",
    "    H_prev, W, b = linear_memory\n",
    "    gradients[\"dH\" + str(L-1)] = np.dot(linear_memory[1].T, dZ)\n",
    "    gradients[\"dW\" + str(L)] = (1. / m) * np.dot(dZ, H_prev.T) \n",
    "    gradients[\"db\" + str(L)] = (1. / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "# Perform the backpropagation l-1 times\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "\n",
    "# Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
    "        current_memory = memories[l]\n",
    "        \n",
    "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)], current_memory, activation=\"relu\")\n",
    "        gradients[\"dH\" + str(l)] = dH_prev_temp\n",
    "        gradients[\"dW\" + str(l + 1)] = dW_temp\n",
    "        gradients[\"db\" + str(l + 1)] = db_temp\n",
    " \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "# parameters is the python dictionary containing the parameters W and b for all the layers\n",
    "# gradients is the python dictionary containing your gradients, output of L_model_backward\n",
    "# returns updated weights after applying the gradient descent update\n",
    " \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    " \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * gradients[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * gradients[\"db\" + str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    \n",
    "    # X and Y are the input training datasets\n",
    "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
    "    # returns updated parameters\n",
    " \n",
    "    np.random.seed(2)\n",
    "    losses = []                         # keep track of loss\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initialize_parameters(dimensions)\n",
    " \n",
    "    for i in range(0, num_iterations):\n",
    " \n",
    "        # Forward propagation\n",
    "        HL, memories = L_layer_forward(X, parameters)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(HL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        gradients = L_layer_backward(HL, Y, memories)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "                \n",
    "        # Printing the loss every 100 training example\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
    "            losses.append(loss)\n",
    "            \n",
    "    # plotting the loss\n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \n",
    "    # Performs forward propagation using the trained parameters and calculates the accuracy\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_layer_forward(X, parameters)\n",
    "    \n",
    "    p = np.argmax(probas, axis = 0)\n",
    "    act = np.argmax(y, axis = 0)\n",
    " \n",
    "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 5000)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x_new = train_set_x[:,0:5000]\n",
    "train_set_y_new = train_set_y[:,0:5000]\n",
    "train_set_x_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1126\n",
      "Accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6fb4953970>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQzElEQVR4nO3de7CU9X3H8fcnXMQYLRIFuZhoqbVxojUdxmaESVBrVIwBx0GDo8EhLekYR5mxWkZHQWtSdRqrtU4mx2JFTbUZEGUsURkn8VKmFnSQS6jxAuoJB5B4AStFLt/+sc9xluPuc87ZffYCv89rZmd3n+9z+Z6Fzz633X0UEZjZge9zrW7AzJrDYTdLhMNulgiH3SwRDrtZIhx2s0Q47PspSb+W9JdFTyvpOkn/UuN8/17SrFqm7edyviPpkUYv50DjsLeYpA2S/qLVfXSLiB9HRL/fRCQdCXwP+Fn2fLCkBdnfF5Im9nN+v5L0rqRtkl6RNLmsx8XAVyWd1N8+U+awW1EuA5ZExI6yYS8AlwCbapjfVcDIiDgMmAk8JGlkWf3hbLj1kcPepiQdLumJbO32fvZ4TI/Rxkr6b0kfSnpc0rCy6b8uaZmkD7I148Q+LneupIeyx0MkPSTp99l8lksaUWXSc4Bnu59ExCcRcWdEvADs6c/fnk2/KiJ2dz8FBgFHl43ya+Dc/s43ZQ57+/oc8K/Al4EvATuAf+4xzveAGcAoYDfwTwCSRgP/AdwCDAP+BliYbWr3x3TgDyiF7IvAX2d9VHIi8Go/558re4P7P+BFSuFeUVZeBxwj6bAil3kgc9jbVET8PiIWRsTHEbEd+BHwzR6jPRgRayLif4EbgAslDaC06bwkIpZExN6IWEopKJP62cYuSiH/o4jYExEvRcS2KuMOBbb3c/65IuLbwKGU+n4qIvaWlbuXNbTIZR7IHPY2Jenzkn4m6S1J24DngKFZmLu9U/b4LUqbukdQ2hqYmm16fyDpA2ACUL7P2xcPAk8Bj0jaKOl2SYOqjPs+pWAWKiJ2RcQvgbMkfaes1L2sD4pe5oHKYW9fVwPHA3+eHaT6RjZcZeOU78N+idKaeCulN4EHI2Jo2e2QiLi1Pw1kQbspIk4ATgW+TWnXoZJVwB/3Z/79NBAYW/b8K8CGnC0N68Fhbw+DsoNh3beBlNZcO4APsgNvcypMd4mkEyR9HrgZWBARe4CHgPMknSVpQDbPiRUO8OWSdJqkE7OtiW2U3kyqHWxbQo/dDEkHSRqSPR2c9aGsdpmkDVWW+yeSzpF0sKRBki6h9Gb3bNlo3wR+2Z+/J3UOe3tYQinY3be5wJ3AwZTW1P8FPFlhugeB+ymd2hoCXAkQEe8Ak4HrgHcpremvof//3kcBCygFfR2lsD1UZdwHgEmSDi4b9mr294ymtDuwg9IuBpS2Sv6zyrxE6TXYkvV/FXBRRLxcNs40snP61jfyj1dYUST9GNgSEXf2YdyngasiYl0NyzkPuDQiLux/l+ly2M0S4c14s0Q47GaJcNjNEjGwmQuT5AMEZg0WEao0vK41u6SzJb0q6XVJs+uZl5k1Vs1H47MPWvwWOBPoBJYD0yLiNznTeM1u1mCNWLOfArweEW9GxCfAI5Q+yGFmbaiesI9m3y9idGbD9iFppqQVklb0rJlZ89RzgK7SpsJnNtMjogPoAG/Gm7VSPWv2Tvb91tUYYGN97ZhZo9QT9uXAcZKOlTQY+C6wuJi2zKxoNW/GR8RuSVdQ+jbTAOC+iFhbWGdmVqimfhHG++xmjdeQD9WY2f7DYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZImq+ZLO1j6lTp1at3XvvvbnTHnbYYbl1qeIFQT81b9683PrVV19dtfbhhx/mTmvFqivskjYA24E9wO6IGFdEU2ZWvCLW7KdFxNYC5mNmDeR9drNE1Bv2AJ6W9JKkmZVGkDRT0gpJK+pclpnVod7N+PERsVHScGCppP+JiOfKR4iIDqADQFLUuTwzq1Fda/aI2JjdbwEWAacU0ZSZFa/msEs6RNKh3Y+BbwFrimrMzIpVz2b8CGBRdh52IPBvEfFkIV3ZPm644Ybc+rXXXlu11tnZmTvt/Pnzc+ubNm3KrV9zzTW59WOPPbZqbcqUKbnTbt++Pbdu/VNz2CPiTeBPC+zFzBrIp97MEuGwmyXCYTdLhMNulgiH3SwR/orrfmDr1vzvGT322GNVazNmzMiddteuXbW09KlRo0bl1i+//PKqtbPOOit32gULFtTUk1XmNbtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgifZ98P9Hae/eKLL65aW79+fe60N954Y009dbv77rtz63nn2a25vGY3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLh77PvB5YuXZpbz/v99bfffrvodvYxePDghs7fitPrml3SfZK2SFpTNmyYpKWSXsvuD29sm2ZWr75sxt8PnN1j2GzgmYg4Dngme25mbazXsEfEc8B7PQZPBuZnj+cDU4pty8yKVus++4iI6AKIiC5Jw6uNKGkmMLPG5ZhZQRp+gC4iOoAOAEnR6OWZWWW1nnrbLGkkQHa/pbiWzKwRag37YmB69ng68Hgx7ZhZoygif8ta0sPAROAIYDMwB3gM+AXwJeBtYGpE9DyIV2le3ow/wCxbtiy3PnTo0Kq1M844I3farq6uWlpKXkSo0vBe99kjYlqVUv6/lJm1FX9c1iwRDrtZIhx2s0Q47GaJcNjNEuGvuFquRYsW5dZPPPHE3PqECROq1nxqrbm8ZjdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuHz7Ae4gQPz/4lvuumm3PrkyZNz63PmzMmtv/LKK7l1ax6v2c0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRPT6U9KFLsw/Jd0Qs2bNqlq74IILcqc99dRTc+sLFy7MrV966aW59Z07d+bWrXjVfkraa3azRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBH+Pvt+YOrUqbn1O+64o2HLliqesu1z3dpHr2t2SfdJ2iJpTdmwuZJ+J2lldpvU2DbNrF592Yy/Hzi7wvB/jIiTs9uSYtsys6L1GvaIeA54rwm9mFkD1XOA7gpJq7LN/MOrjSRppqQVklbUsSwzq1OtYf8pMBY4GegCflJtxIjoiIhxETGuxmWZWQFqCntEbI6IPRGxF7gXOKXYtsysaDWFXdLIsqfnA2uqjWtm7aHX8+ySHgYmAkdI6gTmABMlnQwEsAH4QeNatNWrV+fWr7/++prnPXbs2Nz6jBkzcutDhgzJrc+ePbtqbe3atbnTWrF6DXtETKsweF4DejGzBvLHZc0S4bCbJcJhN0uEw26WCIfdLBH+KWnLdf755+fW583LPzHz0UcfVa2ddtppudO+8cYbuXWrzD8lbZY4h90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwufZrS5nnnlmbv3JJ5+sWrvnnntyp73yyitr6il1Ps9uljiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXC59mtLgMGDMitL1u2rGrtpJNOyp121KhRufX3338/t54qn2c3S5zDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLRl0s2Hw08ABwF7AU6IuIuScOAfweOoXTZ5gsjwic+E7Nnz57c+u7du6vWOjs7c6fduXNnTT1ZZX1Zs+8Gro6IrwBfB34o6QRgNvBMRBwHPJM9N7M21WvYI6IrIl7OHm8H1gGjgcnA/Gy0+cCUBvVoZgXo1z67pGOArwEvAiMiogtKbwjA8MK7M7PC9LrP3k3SF4CFwKyI2CZV/PhtpelmAjNra8/MitKnNbukQZSC/vOIeDQbvFnSyKw+EthSadqI6IiIcRExroiGzaw2vYZdpVX4PGBdRNxRVloMTM8eTwceL749MytKXzbjxwOXAqslrcyGXQfcCvxC0veBt4GpDekwAVOmTMmtDxyY/8+0YMGCArvpn/Hjx+fWjz/++Kq1W265JXfajz/+uKaerLJewx4RLwDVdtDPKLYdM2sUf4LOLBEOu1kiHHazRDjsZolw2M0S4bCbJaLPH5e1xtm0aVNu/bzzzsut13Oe/cgjj8yt33zzzbn1iy66KLe+fv36qrXFixfnTmvF8prdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEz7O3geXLl+fWzz333Nz6888/X7V20EEH5U47ZsyY3PrQoUNz67fddltu/fbbb69a27FjR+60Viyv2c0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRCgimrcwqXkLO4AMH55/Gb2Ojo6qtdNPPz132oULF+bW77rrrtz6ypUrc+vWfBFR8affvWY3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLR63l2SUcDDwBHAXuBjoi4S9Jc4K+Ad7NRr4uIJb3My+fZzRqs2nn2voR9JDAyIl6WdCjwEjAFuBD4KCL+oa9NOOxmjVct7L3+Uk1EdAFd2ePtktYBo4ttz8warV/77JKOAb4GvJgNukLSKkn3STq8yjQzJa2QtKK+Vs2sHn3+bLykLwDPAj+KiEcljQC2AgH8HaVN/Rm9zMOb8WYNVvM+O4CkQcATwFMRcUeF+jHAExHx1V7m47CbNVjNX4SRJGAesK486NmBu27nA2vqbdLMGqcvR+MnAM8DqymdegO4DpgGnExpM34D8IPsYF7evLxmN2uwujbji+KwmzWev89uljiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEtHrD04WbCvwVtnzI7Jh7ahde2vXvsC91arI3r5crdDU77N/ZuHSiogY17IGcrRrb+3aF7i3WjWrN2/GmyXCYTdLRKvD3tHi5edp197atS9wb7VqSm8t3Wc3s+Zp9ZrdzJrEYTdLREvCLulsSa9Kel3S7Fb0UI2kDZJWS1rZ6uvTZdfQ2yJpTdmwYZKWSnotu694jb0W9TZX0u+y126lpEkt6u1oSb+StE7SWklXZcNb+trl9NWU163p++ySBgC/Bc4EOoHlwLSI+E1TG6lC0gZgXES0/AMYkr4BfAQ80H1pLUm3A+9FxK3ZG+XhEfG3bdLbXPp5Ge8G9VbtMuOX0cLXrsjLn9eiFWv2U4DXI+LNiPgEeASY3II+2l5EPAe812PwZGB+9ng+pf8sTVelt7YQEV0R8XL2eDvQfZnxlr52OX01RSvCPhp4p+x5J+11vfcAnpb0kqSZrW6mghHdl9nK7oe3uJ+eer2MdzP1uMx427x2tVz+vF6tCHulS9O00/m/8RHxZ8A5wA+zzVXrm58CYyldA7AL+Ekrm8kuM74QmBUR21rZS7kKfTXldWtF2DuBo8uejwE2tqCPiiJiY3a/BVhEabejnWzuvoJudr+lxf18KiI2R8SeiNgL3EsLX7vsMuMLgZ9HxKPZ4Ja/dpX6atbr1oqwLweOk3SspMHAd4HFLejjMyQdkh04QdIhwLdov0tRLwamZ4+nA4+3sJd9tMtlvKtdZpwWv3Ytv/x5RDT9BkyidET+DeD6VvRQpa8/BF7Jbmtb3RvwMKXNul2Utoi+D3wReAZ4Lbsf1ka9PUjp0t6rKAVrZIt6m0Bp13AVsDK7TWr1a5fTV1NeN39c1iwR/gSdWSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaI/wcFMUSW3l1oOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_train = predict(train_set_x_new, train_set_y_new, parameters)\n",
    "\n",
    "pred_test = predict(test_set_x, test_set_y, parameters)\n",
    "\n",
    "index  = 3476\n",
    "k = test_set_x[:,index]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FeedForward-Numpy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
